# Import raw data
data <- readxl::read_xlsx("../demo_data.xlsx")
# Show questions
names(data)
# Transform into clean format
new_data <- data %>%
rename("user" = 1,
"q1_position_in_trust" = 2,
"q2_rsc_office_met" = 3,
"q12_main_challenges" = 4,
"q15_not_expectations" = 5,
"q22_changes" = 6,
"q24_reflections" = 7)
# Load dplyr to assist
library(dplyr)
# Load surveyr package
devtools::load_all(export_all = FALSE)
# Import raw data
data <- readxl::read_xlsx("../demo_data.xlsx")
# Show questions
names(data)
# Transform into clean format
new_data <- data %>%
rename("user" = 1,
"q1_position_in_trust" = 2,
"q2_rsc_office_met" = 3,
"q12_main_challenges" = 4,
"q15_not_expectations" = 5,
"q22_changes" = 6,
"q24_reflections" = 7)
# Focussing on Q24: Do you have any other reflections about your meetings with your RSC office?
# Take a look at some of the data for comparisons later
sample <- 27:37
# Default behaviour for rest of the demo
anon_data <- new_data %>%
anonymise(q24_reflections)
# clean_column() -----------------------------------------------------------------------------------------
# After anonymising, column needs to be standardised (lower case, punctuation etc.) before it can be analysed
clean_data <- anon_data %>%
clean(q24_reflections)
devtools::load_all(".")
data <- new_data
column <- quo(q24_other_reflections)
cols <- quos(q2_rsc_office_met)
n <- 2
min <- 2
stop_thresh <- 0.7
remove <- c("")
n_grams <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id) %>%
tidytext::unnest_tokens(input = col,
output = "ngram",
token = "ngrams",
n = n) %>%
group_by(!!!cols, ngram) %>%
summarise(Count=n(),
resp_mention = n_distinct(response_id)) %>%
group_by(!!!cols) %>%
mutate(proportion = resp_mention/)
words <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id)
column <- quo(q24_reflections)
words <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id)
words <- words %>%
tidytext::unnest_tokens(input = col,
output = "word",
token = "words") %>%
group_by(!!!cols, word) %>%
summarise(count = n(),
responses = n_distinct(response_id)) %>%
arrange(!!!cols, -count) %>%
filter(!word %in% remove)
c_w <- words %>%
filter(count>min) %>%
slice(1:n) %>%
arrange(!!!cols, -count) %>%
rename("Word" = word,
"Count" = count)
groupsizes <- data %>%
count(!!!cols)
c_w <- c_w %>%
left_join(groupsizes, by = !!!cols)
c_w <- c_w %>%
left_join(groupsizes)
c_w <- c_w %>%
left_join(groupsizes) %>%
mutate(Proportion = round(responses/n, digits = 2)) %>%
select(-responses, -n)
View(c_w)
words <- words %>%
filter(!word %in% tm::stopwords("en"))
c_w <- words %>%
filter(count>min) %>%
slice(1:n) %>%
arrange(!!!cols, -count) %>%
rename("Word" = word,
"Count" = count)
groupsizes <- data %>%
count(!!!cols)
c_w <- c_w %>%
left_join(groupsizes) %>%
mutate(Proportion = round(responses/n, digits = 2)) %>%
select(-responses, -n)
View(c_w)
n_grams <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id) %>%
tidytext::unnest_tokens(input = col,
output = "ngram",
token = "ngrams",
n = n) %>%
group_by(!!!cols, ngram) %>%
summarise(Count=n(),
resp_mention = n_distinct(response_id))
words = 2
n = 5
min = 2
filter_word = ""
n_grams <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id) %>%
tidytext::unnest_tokens(input = col,
output = "ngram",
token = "ngrams",
n = words) %>%
group_by(!!!cols, ngram) %>%
summarise(Count=n(),
resp_mention = n_distinct(response_id)) %>%
filter(count>min) %>%
slice(1:n) %>%
arrange(!!!cols, -count) %>%
rename("Word" = word,
"Count" = count)
n_grams <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id) %>%
tidytext::unnest_tokens(input = col,
output = "ngram",
token = "ngrams",
n = words) %>%
group_by(!!!cols, ngram) %>%
summarise(Count=n(),
resp_mention = n_distinct(response_id)) %>%
filter(Count>min) %>%
slice(1:n) %>%
arrange(!!!cols, -count)
n_grams <- data %>%
mutate(response_id = 1:nrow(data)) %>%
select(col = {{ column }}, !!!cols, response_id) %>%
tidytext::unnest_tokens(input = col,
output = "ngram",
token = "ngrams",
n = words) %>%
group_by(!!!cols, ngram) %>%
summarise(Count=n(),
resp_mention = n_distinct(response_id)) %>%
filter(Count>min) %>%
slice(1:n) %>%
arrange(!!!cols, -Count)
View(n_grams)
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections) # defaults to bigrams
# anonymise() ---------------------------------------------------------------------------------------
# See names 'Joe Bloggs' and 'John Doe' - column needs to be anonymised
# Default behaviour
anon_data <- new_data %>%
anonymise(q24_reflections)
anon_data[sample,"q24_reflections"]
# If we want to distinguish between the different people (i.e. for analysis, see if one person mentioned alot etc.)
anon_data <- new_data %>%
anonymise(q24_reflections,
identify = TRUE)
anon_data[sample,"q24_reflections"] # Random numbers assigned
# If we want to make the column gender neutral as well
anon_data <- new_data %>%
anonymise(q24_reflections,
identify = TRUE,
gender = TRUE)
anon_data[sample,"q24_reflections"]
# If we want to anonymise certain names (i.e. we have a list of certain names)
names_to_anonymise <- c("Joe Bloggs", "the RSC")
anon_data <- new_data %>%
anonymise(q24_reflections,
auto = FALSE, # So that the function doesn't automatically overwrite every name it finds
add_names = names_to_anonymise, # Manually defined names to anonymise
identify = TRUE,
gender = TRUE)
anon_data[sample,"q24_reflections"]
# Can also apply to several/all columns via anonymise_at and anonymise_all
anon_df <- new_data %>%
anonymise_at(q24_reflections,
q22_changes,
add_names = "RSC",
identify = TRUE)
anon_df <- new_data %>%
anonymise_all(quietly = TRUE)
# Default behaviour for rest of the demo
anon_data <- new_data %>%
anonymise(q24_reflections)
anon_data[sample,"q24_reflections"]
# clean_column() -----------------------------------------------------------------------------------------
# After anonymising, column needs to be standardised (lower case, punctuation etc.) before it can be analysed
clean_data <- anon_data %>%
clean(q24_reflections)
clean_data[sample,"q24_reflections"]
# If we want to keep 'null responses'
clean_data <- anon_data %>%
clean(q24_reflections,
null_response = FALSE)
clean_data[sample,"q24_reflections"]
# Again, can apply to multiple columns easily
clean_data <- anon_data %>%
clean_at(q24_reflections,
q22_changes,
q12_main_challenges)
clean_data <- anon_data %>%
clean_all()
# common_words() --------------------------------------------------------------------------------------------
# Simple frequency analysis, filters out words that appear less than 5 times so
# filters out groups with only a couple of respondents
clean_data %>%
common_words(q24_reflections)
# More words
clean_data %>%
common_words(q24_reflections,
n = 10)
# Remove certain words
clean_data %>%
common_words(q24_reflections,
remove = c("meeting", "rsc", "trust"),
n = 10)
# Breakdown by demographic
clean_data %>%
common_words(q24_reflections,
q2_rsc_office_met,
remove = c("meeting", "rsc", "trust"),
n = 1)
# To include all groups
clean_data %>%
common_words(q24_reflections,
q2_rsc_office_met,
remove = c("meeting", "rsc", "trust"),
n = 5) # Top 5 words but some are being excluded because they occur less than 5 times
clean_data %>%
common_words(q24_reflections,
q2_rsc_office_met,
remove = c("meeting", "rsc", "trust"),
n = 5,
min = 0) # include top five words regardless of how often they occur
# Stopwords are currently being removed, to keep them in
clean_data %>%
common_words(q24_reflections,
q2_rsc_office_met,
remove_stopwords = FALSE)
# Can also calculate the proportion of responses in each group
# that mentioned the word
clean_data %>%
common_words(q24_reflections,
q2_rsc_office_met,
remove = c("meeting", "rsc", "trust"),
n = 1,
proportion = TRUE)
# Finally, if exporting to external document, can make it more visually
# appealing with pretty='plot' argument
clean_data %>%
common_words(q24_reflections,
q2_rsc_office_met,
proportion = TRUE,
remove = c("meeting", "rsc", "trust"),
n = 2,
pretty = 'return')
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections) # defaults to bigrams
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
n_g <- n_grams %>%
left_join(datagroups, by = quo_names(cols)) %>%
mutate(Proportion = round(resp_mention/n,
digits = 2)) %>%
select(-resp_mention, -n)
datagroups <- data %>%
count(...)
datagroups <- data %>%
count(cols)
cols <- quos(q1_position_in_trust)
datagroups <- data %>%
count(cols)
cols
datagroups <- data %>%
count(...)
datagroups <- data %>% count(q1_position_in_trust)
n_g <- n_grams %>%
left_join(datagroups, by = quo_names(cols)) %>%
mutate(Proportion = round(resp_mention/n,
digits = 2)) %>%
select(-resp_mention, -n)
n_g <- n_grams %>%
left_join(datagroups, by = quo_name(cols)) %>%
mutate(Proportion = round(resp_mention/n,
digits = 2)) %>%
select(-resp_mention, -n)
n_g <- n_grams %>%
left_join(datagroups, by = cols) %>%
mutate(Proportion = round(resp_mention/n,
digits = 2)) %>%
select(-resp_mention, -n)
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
devtools::load_all(".")
warnings()
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting"),
min = 3) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting"),
min = 3) # defaults to bigrams
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting"),
min = 0) # defaults to bigrams
remove = c("rsc", "trust", "meeting")
paste(remove, collapse = "|")
paste0("(", paste(remove, collapse = ")|("), ")")
warnings()
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting")) # defaults to bigrams
cols <- quos(q1_position_in_trust)
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting")) # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting")) # defaults to bigrams
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting"),
pretty = 'plot') # defaults to bigrams
devtools::load_all(".")
# n_grams() --------------------------------------------------------------------------------------------------
# Method of tabulating most common n-grams, removing n_grams that are mostly stopwords
clean_data %>%
n_grams(q24_reflections,
q1_position_in_trust,
remove = c("rsc", "trust", "meeting"),
pretty = 'plot') # defaults to bigrams
library(surveyr)
getwd()
?install.packages
install.packages("C:/Users/cbrownlie/OneDrive - Department for Education/Documents/Labs 2019/surveyr_0.0.4.zip", repos = NULL)
rm(surveyr)
# Load dplyr to assist
library(dplyr)
# Import raw data
data <- readxl::read_xlsx("../demo_data.xlsx")
# Show questions
names(data)
# Transform into clean format
new_data <- data %>%
rename("user" = 1,
"q1_position_in_trust" = 2,
"q2_rsc_office_met" = 3,
"q12_main_challenges" = 4,
"q15_not_expectations" = 5,
"q22_changes" = 6,
"q24_reflections" = 7)
# Load surveyr package
devtools::load_all(export_all = FALSE)
# Default behaviour for rest of the demo
anon_data <- new_data %>%
anonymise(q24_reflections)
clean_data <- anon_data %>%
clean_all()
num_topics = 2
dataframe <- clean_data
column <- quo(q12_main_challenges)
num_words = 3
num_topics = 2
exclude = ""
beta_matrix <- determine_topics(dataframe = dataframe,
column = enquo(column),
num_topics = num_topics) %>%
tidytext::tidy(matrix = "beta")
#' Function which takes in a dataframe and column and abstracts the use of
#' the topicmodels package by applying an LDA model and saving data internally
#' to be used later by other *_topics functions
#'
#'
#' @param dataframe dataframe or tibble of survey responses
#' @param column string variable of free text responses to which the LDA model will be applied
#' @param num_topics integer denoting the number of topics (k) to be specified for the LDA model, defaults to 2
#'
#' @return a dataframe, where each column is a topic and contains the 5 most common words in those topics
determine_topics <- function(dataframe,
column,
num_topics = 2) {
if (class(column)[1] != "quosure") {
column <- enquo(column)
}
model_matrix <- dataframe %>%
mutate(id = seq_along(pull(., 1))) %>%
select(id, quo_name(column)) %>%
tidytext::unnest_tokens(output = "word",
input = {{ column }},
token = "words") %>%
group_by(id, word) %>%
summarise(count = n()) %>%
tidytext::cast_dtm(document = "id",
term = "word",
value = "count") %>%
topicmodels::LDA(k = num_topics, control = list(seed = 1066))
return(model_matrix)
}
beta_matrix <- determine_topics(dataframe = dataframe,
column = enquo(column),
num_topics = num_topics) %>%
tidytext::tidy(matrix = "beta")
topic_words_initial <- beta_matrix %>%
filter(!term %in% tm::stopwords("en")) %>%
group_by(topic) %>%
top_n(n = 5*num_words, wt = beta)
View(topic_words_initial)
topic_words_initial <- beta_matrix %>%
filter(!term %in% tm::stopwords("en")) %>%
group_by(topic) %>%
top_n(n = 5, wt = beta)
?n_grams
?anonymise
